{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a009baa-f129-4193-bee3-b25601d6b0a5",
   "metadata": {},
   "source": [
    "- MP -> MDP\n",
    "    - MP: markov process\n",
    "        - 即系统的当前状态只与前一个状态有关，与更早之前的历史状态无关。换句话说，系统的未来发展只依赖于当前状态，而与过去的状态没有直接关系。\n",
    "    - MDP：markov decision process\n",
    "        - 马尔可夫决策过程是在马尔可夫过程的基础上增加了决策的元素。除了状态转移外，还考虑了**行动Action**的选择和**奖励Reward**的反馈。\n",
    "        - MDP有几个主要组成部分：\n",
    "            - 状态（State, S）：描述系统的某一特定情况。\n",
    "            - 动作（Action, A）：在某一状态下可以选择的行动。\n",
    "            - 转移概率（Transition Probability, P）：在某个状态下采取某个动作后转移到另一个状态的概率。\n",
    "            - 奖励（Reward, R）：每次从一个状态到另一个状态的转移后获得的奖励。\n",
    "            - 折扣因子（Discount Factor, γ）：用于衡量未来奖励的价值，通常是一个小于1的值，表示未来的奖励比当前的奖励值要低。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
