{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a009baa-f129-4193-bee3-b25601d6b0a5",
   "metadata": {},
   "source": [
    "- MP -> MDP\n",
    "    - MP: markov process\n",
    "        - 即系统的当前状态只与前一个状态有关，与更早之前的历史状态无关。换句话说，系统的未来发展只依赖于当前状态，而与过去的状态没有直接关系。\n",
    "    - MDP：markov decision process\n",
    "        - 马尔可夫决策过程是在马尔可夫过程的基础上增加了决策的元素。除了状态转移外，还考虑了**行动Action**的选择和**奖励Reward**的反馈。\n",
    "        - MDP有几个主要组成部分：\n",
    "            - 状态（State, S）：描述系统的某一特定情况。\n",
    "            - 动作（Action, A）：在某一状态下可以选择的行动。\n",
    "            - 转移概率（Transition Probability, P）：在某个状态下采取某个动作后转移到另一个状态的概率。\n",
    "            - 奖励（Reward, R）：每次从一个状态到另一个状态的转移后获得的奖励。\n",
    "            - 折扣因子（Discount Factor, γ）：用于衡量未来奖励的价值，通常是一个小于1的值，表示未来的奖励比当前的奖励值要低。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0602092-4247-4115-816b-46a3b3e5876f",
   "metadata": {},
   "source": [
    "### 马尔科夫性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abe0f9-0d27-4b88-9bd8-074991d298a9",
   "metadata": {},
   "source": [
    "$$\n",
    "p(s'|s,a)=p(s_{t+1}|s_t,a_t)\n",
    "$$\n",
    "\n",
    "- 下一时刻的状态转移（转移到什么状态 $s_{t+1}$，只跟当前时间的状态 $s_t$ 以及当前时刻采取的动作 $a_t$ 有关，而与更早之前的状态无关；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
