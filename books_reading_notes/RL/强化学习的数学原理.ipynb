{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af5d945-fe60-488e-86aa-ed412b9d0c43",
   "metadata": {},
   "source": [
    "- chap1：基本概念\n",
    "- chap2-chap8：value-based methods\n",
    "    - 先估计值再得到最优策略；\n",
    "- chap9-10：policy-based methods，直接优化关于策略参数 $\\pi_\\theta(a|s)$ 的目标函数\n",
    "    \n",
    "    - chap9：PG（policy gradient）\n",
    "        - 策略梯度定理\n",
    "            - https://www.datacamp.com/tutorial/policy-gradient-theorem\n",
    "    - chap10：Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574dc59f-fa8e-4eb7-a160-7d847c56eab5",
   "metadata": {},
   "source": [
    "### PG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77f626-235d-4e82-87a9-72ec8012d2a7",
   "metadata": {},
   "source": [
    "$$\n",
    "J(θ) = E[G_t]=E[G_0]\n",
    "$$\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Policy_gradient_method\n",
    "    - log-derivate trick：方便整理成策略的期望形式\n",
    "        - $f'=f\\nabla \\log f=f\\left(\\frac{f'}{f}\\right)$\n",
    "        - $\\nabla\\pi_\\theta=\\pi_\\theta\\nabla_\\theta\\log\\pi_\\theta$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta) &\\propto \\sum_s d^{\\pi_\\theta}(s) \\sum_a (\\nabla_\\theta \\pi_\\theta(a|s)) Q^{\\pi_\\theta}(s,a) \\\\\n",
    "&\\quad \\text{(An intermediate step in Policy Gradient theorem derivation)} \\\\\n",
    "\\\\\n",
    "&\\text{The log-derivative trick states: } \\nabla_x f(x) = f(x) \\nabla_x \\log f(x). \\\\\n",
    "&\\text{Applying this with } f(x) = \\pi_\\theta(a|s) \\text{ (the policy) and variable } \\theta: \\\\\n",
    "\\nabla_\\theta \\pi_\\theta(a|s) &= \\pi_\\theta(a|s) \\nabla_\\theta \\log \\pi_\\theta(a|s). \\\\\n",
    "\\\\\n",
    "&\\text{Substituting this identity back into the expression for } \\nabla_\\theta J(\\theta): \\\\\n",
    "\\nabla_\\theta J(\\theta) &\\propto \\sum_s d^{\\pi_\\theta}(s) \\sum_a \\left( \\pi_\\theta(a|s) \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right) Q^{\\pi_\\theta}(s,a) \\\\\n",
    "&= \\sum_s d^{\\pi_\\theta}(s) \\sum_a \\pi_\\theta(a|s) \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) Q^{\\pi_\\theta}(s,a) \\right] \\\\\n",
    "&= \\mathbb{E}_{\\substack{s \\sim d^{\\pi_\\theta} \\\\ a \\sim \\pi_\\theta(\\cdot|s)}} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) Q^{\\pi_\\theta}(s,a) \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "------\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}{\\substack{s \\sim d^{\\pi\\theta} \\ a \\sim \\pi_\\theta(\\cdot|s)}} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot R(s,a) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "- R 的常见形式及其期望下标：\n",
    "    - $G_t$ (累积回报 Total Return): 这是 REINFORCE 算法中使用的形式。$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t \\right]$\n",
    "    - $Q^{\\pi_\\theta}(s,a)$: $\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\substack{s \\sim d^{\\pi_\\theta} \\\\ a \\sim \\pi_\\theta(\\cdot|s)}} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) Q^{\\pi_\\theta}(s,a) \\right]$\n",
    "    - $A^{\\pi_\\theta}(s,a)$ (优势函数 Advantage Function): $\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\substack{s \\sim d^{\\pi_\\theta} \\\\ a \\sim \\pi_\\theta(\\cdot|s)}} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) A^{\\pi_\\theta}(s,a) \\right]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
